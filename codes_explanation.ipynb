{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bc8e41-0aeb-4d66-b294-260b6fe44575",
   "metadata": {},
   "source": [
    "#对所写的代码的解释：\n",
    "\n",
    "这段代码主要是用来对词向量模型进行初始化、训练和评估，以下是各个部分的简要说明：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "初始化部分：导入所需的库，并定义了一系列函数和类来实现词向量模型的训练、评估和可视化。\n",
    "\n",
    "display函数：用于展示数据。\n",
    "\n",
    "read_json函数：用于从JSON文件中读取数据。\n",
    "\n",
    "select_model函数：根据模型名称选择对应的模型，如Word2Vec或FastText。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Model类：包含了模型的初始化、预训练、训练和评估方法。\n",
    "\n",
    "__init__方法：初始化模型参数。\n",
    "\n",
    "cut_words方法：对文本数据进行分词处理。\n",
    "\n",
    "pre_train_model方法：预训练一个词向量模型。\n",
    "\n",
    "random_samples_trainmodel方法：使用随机采样进行模型训练。\n",
    "\n",
    "epoches_samples_trainmodel方法：将数据分批进行模型训练。\n",
    "\n",
    "prove_model方法：评估模型性能。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "versionism类：包含了对训练好的模型进行可视化和评估的方法。\n",
    "\n",
    "__init__方法：初始化模型。\n",
    "\n",
    "demosion_h22方法：使用t-SNE对词向量进行降维并保存。\n",
    "\n",
    "vector_2d_show方法：展示词向量的二维可视化结果。\n",
    "\n",
    "cpka方法：展示两个词之间的余弦相似度，并在二维可视化图上标记这两个词及其相似词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5de01-0daa-4d7c-93c0-d009af6c73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caec48-118d-4a79-bdf8-7df409e62992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示数据\n",
    "def display(data: list):\n",
    "    print('len:', len(data))\n",
    "    for i in range(5):\n",
    "        print(data[i])\n",
    "\n",
    "# 读取JSON文件并提取特定字符的数据\n",
    "def read_json(path_way, character):\n",
    "    data = []\n",
    "    with open(path_way, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if line != '':\n",
    "                sample = json.loads(line)\n",
    "                data.append(sample.copy())\n",
    "    # 从数据中提取指定的字符数据\n",
    "    train_data = [i[character] for i in data[:]]\n",
    "    return train_data\n",
    "\n",
    "# 选择模型\n",
    "def select_model(model_name):\n",
    "    if model_name.lower() == 'word2vec':\n",
    "        return Word2Vec\n",
    "    elif model_name.lower() == 'fasttext':\n",
    "        return FastText\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6882315-e704-49ef-abfe-f91b74da4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_name, args, model_path):#设置参数，数据，模型信息\n",
    "        \"\"\"\n",
    "        初始化Model对象。\n",
    "\n",
    "        参数：\n",
    "            - model_name (str): 模型名称，支持'word2vec'或'fasttext'。\n",
    "            - args (dict): 模型参数。\n",
    "            - model_path (str): 模型保存路径。\n",
    "\n",
    "        \"\"\"\n",
    "        self.paras = args\n",
    "        self.model_path = model_path\n",
    "        self.model = select_model(model_name)  # 根据模型名称选择模型\n",
    "        \n",
    "    def cut_words(self, train_data):\n",
    "        \"\"\"\n",
    "        对文本进行分词处理。\n",
    "\n",
    "        参数：\n",
    "            - train_data (list): 待分词的文本数据。\n",
    "\n",
    "        返回：\n",
    "            - seg_data (list): 分词后的文本数据。\n",
    "\n",
    "        \"\"\"\n",
    "        seg_data = list()\n",
    "        for i in train_data:\n",
    "            new_text = \"\".join(re.findall('[\\u4e00-\\u9fa5]+', i, re.S))  # 去除一些无用的字符只提取出中文出来\n",
    "            seg_i = jieba.lcut(new_text)\n",
    "            seg_data.append(seg_i)\n",
    "        return seg_data\n",
    "    \n",
    "    def pre_train_model(self, data):\n",
    "        \"\"\"\n",
    "        预训练一个模型。\n",
    "\n",
    "        参数：\n",
    "            - data (list): 训练数据。\n",
    "\n",
    "        \"\"\"\n",
    "        seg_data = self.cut_words(data[:1000])  # 只使用前1000条数据进行预训练\n",
    "        Model = self.model(sentences=seg_data, **self.paras)  # 根据参数初始化模型\n",
    "        Model.save(self.model_path)  # 保存模型\n",
    "    \n",
    "    def random_samples_trainmodel(self, data, sample_size, epoches):\n",
    "        \"\"\"\n",
    "        使用随机采样进行模型训练。\n",
    "\n",
    "        参数：\n",
    "            - data (list): 训练数据。\n",
    "            - sample_size (int): 每次采样的样本数量。\n",
    "            - epoches (int): 训练轮数。\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            self.pre_train_model(data)  # 如果模型不存在，则预训练一个模型\n",
    "        Model = self.model.load(self.model_path)  # 加载预训练的模型\n",
    "        with tqdm(total=epoches, desc=\"Training\", unit=\"step\") as pbar:\n",
    "            for _ in range(epoches):\n",
    "                random_samples = random.sample(data, sample_size)  # 随机采样\n",
    "                seg_data = self.cut_words(random_samples)  # 分词处理\n",
    "                Model.train(seg_data, total_examples=len(seg_data), epochs=1)  # 训练模型\n",
    "                pbar.update(1)  # 更新进度条\n",
    "            Model.save(self.model_path)  # 保存模型\n",
    "    \n",
    "    def epoches_samples_trainmodel(self, data, delt_data, size):\n",
    "        \"\"\"\n",
    "        使用分批数据进行模型训练。\n",
    "\n",
    "        参数：\n",
    "            - data (list): 训练数据。\n",
    "            - delt_data (int): 每个批次的数据量。\n",
    "            - size (int): 总批次数。\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            self.pre_train_model(data)  # 如果模型不存在，则预训练一个模型\n",
    "        Model = self.model.load(self.model_path)  # 加载预训练的模型\n",
    "        with tqdm(total=size, desc=\"Training\", unit=\"step\") as pbar:\n",
    "            for t in range(1, size + 1):\n",
    "                cut_data = data[delt_data * (t - 1):delt_data * t]  # 切割数据\n",
    "                seg_data = self.cut_words(cut_data)  # 分词处理\n",
    "                Model.train(seg_data, total_examples=len(seg_data), epochs=1)  # 训练模型\n",
    "                pbar.update(1)  # 更新进度条\n",
    "            Model.save(self.model_path)  # 保存模型\n",
    "        \n",
    "    def prove_model(self):\n",
    "        \"\"\"\n",
    "        评估模型性能。\n",
    "\n",
    "        \"\"\"\n",
    "        model = self.model.load(self.model_path)  # 加载模型\n",
    "        print(\"查看词向量大小:\", model.wv.vectors.shape)  # 查看词向量维度\n",
    "        print(\"计算两个词的余弦相似度:\", model.wv.similarity('中国', '美国'))  # 计算两个词的余弦相似度\n",
    "        print(\"取出与“中国”最相似的10个词\", model.wv.most_similar('中国', topn=10))  # 获取与指定词最相似的词汇\n",
    "        print(\"获得 国王-男人+女人 的词，理应为女王，而实际上最接近的10个词为：\",\n",
    "              model.wv.most_similar(positive=[\"男人\", \"国王\"], negative=[\"女人\"], topn=10))  # 进行类比测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0747bca-8e02-48d4-a39d-104b18caeff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class versionism():\n",
    "    def __init__(self, model_name, model_path, save_path):\n",
    "        \"\"\"\n",
    "        初始化versionism对象。\n",
    "\n",
    "        参数：\n",
    "            - model_name (str): 模型名称，支持'word2vec'或'fasttext'。\n",
    "            - model_path (str): 预训练模型的路径。\n",
    "            - save_path (str): 保存降维后词向量的路径。\n",
    "\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        modeltype = select_model(model_name)\n",
    "        self.model = modeltype.load(model_path)\n",
    "        \n",
    "    def demosion_h22(self):\n",
    "        \"\"\"\n",
    "        使用t-SNE算法将词向量降维至二维，并保存降维后的结果。\n",
    "\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        words = list(model.wv.key_to_index)  # 获取模型中所有词语的词向量\n",
    "        vectors = model.wv[words]  # 获取词向量\n",
    "        tsne = TSNE(n_components=2, random_state=42)  # 创建t-SNE对象\n",
    "        vectors_2d = tsne.fit_transform(vectors)  # 将词向量降维至二维\n",
    "        np.save(self.save_path, vectors_2d)  # 保存降维后的结果\n",
    "        print(\"Finished saving vector_2d\")\n",
    "        \n",
    "    def vector_2d_show(self):\n",
    "        \"\"\"\n",
    "        将降维后的词向量进行可视化展示。\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.save_path):\n",
    "            self.demosion_h22()  # 如果降维后的词向量不存在，则进行降维处理\n",
    "         \n",
    "        vectors_2d = np.load(self.save_path)  # 加载降维后的词向量\n",
    "        model = self.model\n",
    "        words = list(model.wv.key_to_index)  # 获取模型中所有词语的词向量\n",
    "\n",
    "        plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "        plt.figure(figsize=(10, 8))  # 创建一个二维可视化图\n",
    "        plt.scatter(vectors_2d[:1000, 0], vectors_2d[:1000, 1], marker='o', s=8)  # 绘制散点图\n",
    "        for i, word in enumerate(words[:1000]):\n",
    "            plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=20)  # 添加词语标签\n",
    "\n",
    "        plt.title('t-SNE Visualization of Word2Vec Embeddings')  # 添加图标题\n",
    "        plt.xlabel('t-SNE Dimension 1')  # 添加x轴标签\n",
    "        plt.ylabel('t-SNE Dimension 2')  # 添加y轴标签\n",
    "\n",
    "        plt.show()  # 显示可视化图\n",
    "        \n",
    "    def cpka(self, word1, word2):\n",
    "        \"\"\"\n",
    "        将两个词汇在二维空间中的位置进行可视化展示，并计算它们的余弦相似度。\n",
    "\n",
    "        参数：\n",
    "            - word1 (str): 第一个词汇。\n",
    "            - word2 (str): 第二个词汇。\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.save_path):\n",
    "            self.demosion_h22()  # 如果降维后的词向量不存在，则进行降维处理\n",
    "        vectors_2d = np.load(self.save_path)  # 加载降维后的词向量 \n",
    "        \n",
    "        model = self.model\n",
    "        words = list(model.wv.key_to_index)  # 获取模型中所有词语的词向量\n",
    "\n",
    "        china = [words.index(word) for word, _ in model.wv.most_similar(positive=[word1], topn=100)]  # 取出与“中国”最相似的100个词\n",
    "        america = [words.index(word) for word, _ in model.wv.most_similar(positive=[word2], topn=100)]  # 取出与“美国”最相似的100个词\n",
    "\n",
    "        plt.figure(figsize=(10, 8))  # 创建一个二维可视化图\n",
    "        plt.scatter(vectors_2d[china, 0], vectors_2d[china, 1], marker='o', s=8)  # 绘制散点图\n",
    "        plt.scatter(vectors_2d[america, 0], vectors_2d[america, 1], marker='o', s=8)  # 绘制散点图\n",
    "        for i in china:\n",
    "            plt.annotate(words[i], xy=(vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=20, color='red')  # 添加词语标签\n",
    "        for i in america:\n",
    "            plt.annotate(words[i], xy=(vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=20, color='blue')  # 添加词语标签\n",
    "        plt.show()  # 显示可视化图\n",
    "\n",
    "        print(\"两个词的余弦相似度:\", model.wv.similarity(word1, word2))  # 计算两个词的余弦相似度\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
